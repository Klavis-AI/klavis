{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klavis-ai/klavis/blob/main/examples/gemini/Use_Klavis_with_Gemini.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Gemini + Klavis AI Integration\n",
        "\n",
        "This tutorial demonstrates how to use Google's Gemini with function calling with Klavis MCP (Model Context Protocol) servers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "- **Google AI API key** - Get at [ai.google.dev](https://ai.google.dev/)\n",
        "- **Klavis API key** - Get at [klavis.ai](https://klavis.ai/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "%pip install -qU google-generativeai klavis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zihaolin/src/klavis/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from klavis import Klavis\n",
        "from klavis.types import McpServerName, ConnectionType, ToolFormat\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\"  # Replace with your actual Google AI API key\n",
        "os.environ[\"KLAVIS_API_KEY\"] = \"YOUR_KLAVIS_API_KEY\"  # Replace with your actual Klavis API key\n",
        "\n",
        "# Configure Gemini\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Case Study 1 : Gemini + YouTube MCP Server\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Step 1 - Create YouTube MCP Server using Klavis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "klavis_client = Klavis(api_key=os.getenv(\"KLAVIS_API_KEY\"))\n",
        "\n",
        "youtube_mcp_instance = klavis_client.mcp_server.create_server_instance(\n",
        "    server_name=McpServerName.YOUTUBE,\n",
        "    user_id=\"1234\",\n",
        "    platform_name=\"Klavis\",\n",
        "    connection_type=ConnectionType.STREAMABLE_HTTP,\n",
        ")\n",
        "\n",
        "# print(f\"🔗 YouTube MCP server created at: {youtube_mcp_instance.server_url}, and the instance id is {youtube_mcp_instance.instance_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Step 2 - Create general method to use MCP Server with Gemini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def gemini_with_mcp_server(mcp_server_url: str, user_query: str):\n",
        "    # Get tools from MCP server\n",
        "    mcp_server_tools = klavis_client.mcp_server.list_tools(\n",
        "        server_url=mcp_server_url,\n",
        "        connection_type=ConnectionType.STREAMABLE_HTTP,\n",
        "        format=ToolFormat.GEMINI,\n",
        "    )\n",
        "    print(mcp_server_tools)\n",
        "    \n",
        "    # Initialize Gemini model with tools\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-flash\",\n",
        "        tools=mcp_server_tools.tools\n",
        "    )\n",
        "    \n",
        "    # Start chat\n",
        "    chat = model.start_chat()\n",
        "    \n",
        "    # Send initial message\n",
        "    response = chat.send_message(user_query)\n",
        "    \n",
        "    print(response)\n",
        "    \n",
        "    # Check if function call is requested\n",
        "    if response.candidates[0].content.parts[0].function_call:\n",
        "        function_call = response.candidates[0].content.parts[0].function_call\n",
        "        function_name = function_call.name\n",
        "        function_args = dict(function_call.args)\n",
        "        \n",
        "        print(f\"🔧 Calling: {function_name}, with args: {function_args}\")\n",
        "        \n",
        "        # Call the MCP server tool\n",
        "        result = klavis_client.mcp_server.call_tools(\n",
        "            server_url=mcp_server_url,\n",
        "            tool_name=function_name,\n",
        "            tool_args=function_args,\n",
        "            connection_type=ConnectionType.STREAMABLE_HTTP\n",
        "        )\n",
        "        \n",
        "        # Send function response back to model\n",
        "        function_response = genai.protos.Part(\n",
        "            function_response=genai.protos.FunctionResponse(\n",
        "                name=function_name,\n",
        "                response={\"result\": result.result.content[0]['text']}\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        final_response = chat.send_message([function_response])\n",
        "        return final_response.text\n",
        "    else:\n",
        "        return response.text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Step 3 - Summarize your favorite video!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "success=True tools=[{'function_declarations': [{'name': 'get_youtube_video_transcript', 'description': \"Retrieve the transcript or video details for a given YouTube video. The 'start' time in the transcript is formatted as MM:SS or HH:MM:SS.\", 'parameters': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The URL of the YouTube video to retrieve the transcript/subtitles for. (e.g. https://www.youtube.com/watch?v=dQw4w9WgXcQ)'}}, 'required': ['url']}}]}] format=<ToolFormat.GEMINI: 'gemini'> error=None\n",
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"function_call\": {\n",
            "                  \"name\": \"get_youtube_video_transcript\",\n",
            "                  \"args\": {\n",
            "                    \"url\": \"https://www.youtube.com/watch?v=LCEmiRjPEtQ\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"index\": 0\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 141,\n",
            "        \"candidates_token_count\": 37,\n",
            "        \"total_token_count\": 378\n",
            "      },\n",
            "      \"model_version\": \"gemini-2.5-flash\"\n",
            "    }),\n",
            ")\n",
            "🔧 Calling: get_youtube_video_transcript, with args: {'url': 'https://www.youtube.com/watch?v=LCEmiRjPEtQ'}\n",
            "Andrej Karpathy's keynote at AI Startup School on June 17, 2025, discusses the fundamental shift in software development, moving from \"Software 1.0\" to \"Software 3.0.\" He posits that Large Language Models (LLMs) represent a new kind of computer, where natural language, specifically English, becomes the primary programming interface.\n",
            "\n",
            "Here's a summary of the video by chapters:\n",
            "\n",
            "*   **00:00 - Intro**: Introduction to the topic of software evolution.\n",
            "*   **01:25 - Software evolution: From 1.0 to 3.0**: Karpathy elaborates on the historical progression of software, leading up to the current era of Software 3.0.\n",
            "*   **04:40 - Programming in English: Rise of Software 3.0**: This section highlights how natural language is becoming the new way to program, enabling LLMs to perform complex tasks.\n",
            "*   **06:10 - LLMs as utilities, fabs, and operating systems**: Karpathy draws analogies, explaining LLMs' roles similar to utilities, fabrication plants (fabs), and even operating systems, suggesting we are in the early stages of computing, akin to the 1960s.\n",
            "*   **11:04 - The new LLM OS and historical computing analogies**: Further discussion on the concept of an \"LLM OS\" and how historical computing paradigms can help understand this new landscape.\n",
            "*   **14:39 - Psychology of LLMs: People spirits and cognitive quirks**: He describes LLMs as \"people spirits\" or stochastic simulations of people, possessing emergent psychology, superhuman abilities in some areas, but also fallibilities.\n",
            "*   **18:22 - Designing LLM apps with partial autonomy**: Given the nature of LLMs, the focus shifts to designing applications that leverage their \"people spirit\" for partial autonomy.\n",
            "*   **23:40 - The importance of human-AI collaboration loops**: Emphasis on the necessity of effective collaboration between humans and AI for productive outcomes.\n",
            "*   **26:00 - Lessons from Tesla Autopilot & autonomy sliders**: Insights gained from developing Tesla Autopilot are applied to the concept of autonomy in LLM applications, including the idea of \"autonomy sliders.\"\n",
            "*   **27:52 - The Iron Man analogy: Augmentation vs. agents**: An analogy to Iron Man is used to differentiate between AI augmenting human capabilities and AI acting as independent agents.\n",
            "*   **29:06 - Vibe Coding: Everyone is now a programmer**: This part discusses how programming in English makes software development highly accessible, leading to \"vibe coding\" where nearly anyone can contribute.\n",
            "*   **33:39 - Building for agents: Future-ready digital infrastructure**: With LLMs as new primary consumers/manipulators of digital information, there's a call to build digital infrastructure that is ready for agents.\n",
            "*   **38:14 - Summary: We’re in the 1960s of LLMs — time to build**: Karpathy concludes by reiterating that the current state of LLMs is comparable to the 1960s of computing, signifying a nascent but rapidly evolving field, and encouraging development.\n",
            "\n",
            "Overall, the video highlights a significant shift in software, where natural language interfaces and LLMs are creating a new computing paradigm, making software development more accessible and enabling new forms of human-AI collaboration and autonomous applications.\n"
          ]
        }
      ],
      "source": [
        "YOUTUBE_VIDEO_URL = \"https://www.youtube.com/watch?v=LCEmiRjPEtQ\"  # pick a video you like!\n",
        "\n",
        "result = gemini_with_mcp_server(\n",
        "    mcp_server_url=youtube_mcp_instance.server_url, \n",
        "    user_query=f\"Please provide a complete summary of this YouTube video with timestamp: {YOUTUBE_VIDEO_URL}\"\n",
        ")\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "✅ Great! You've successfully created an AI agent that uses Gemini's function calling with Klavis MCP servers to summarize YouTube videos!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Case Study 2 : Gemini + Gmail MCP Server (OAuth needed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔐 Opening OAuth authorization for Gmail, if you are not redirected, please open the following URL in your browser: https://api.klavis.ai/oauth/gmail/authorize?instance_id=d9d482b3-433a-4330-9a8b-9548c0b0a326\n"
          ]
        }
      ],
      "source": [
        "import webbrowser\n",
        "\n",
        "# Create Gmail MCP server instance\n",
        "gmail_mcp_server = klavis_client.mcp_server.create_server_instance(\n",
        "    server_name=McpServerName.GMAIL,\n",
        "    user_id=\"1234\",\n",
        "    platform_name=\"Klavis\",\n",
        "    connection_type=ConnectionType.STREAMABLE_HTTP,\n",
        ")\n",
        "\n",
        "# Redirect to Gmail OAuth page for authorization\n",
        "webbrowser.open(gmail_mcp_server.oauth_url)\n",
        "\n",
        "print(f\"🔐 Opening OAuth authorization for Gmail, if you are not redirected, please open the following URL in your browser: {gmail_mcp_server.oauth_url}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EMAIL_RECIPIENT = \"zihaolin@klavis.ai\" # Replace with your email\n",
        "EMAIL_SUBJECT = \"Test Gemini + Gmail MCP Server\"\n",
        "EMAIL_BODY = \"Hello World from Gemini!\"\n",
        "\n",
        "result = gemini_with_mcp_server(\n",
        "    mcp_server_url=gmail_mcp_server.server_url, \n",
        "    user_query=f\"Please send an email to {EMAIL_RECIPIENT} with subject {EMAIL_SUBJECT} and body {EMAIL_BODY}\"\n",
        ")\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This tutorial demonstrated how to integrate Google's Gemini with function calling capabilities with Klavis MCP servers to create powerful AI applications. We covered two practical examples:\n",
        "\n",
        "**🎥 YouTube Integration**: Built an AI assistant that can automatically summarize YouTube videos by extracting transcripts and providing detailed, timestamped summaries.\n",
        "\n",
        "**📧 Gmail Integration**: Created an AI-powered email assistant that can send emails through Gmail with OAuth authentication.\n",
        "\n",
        "### Key Takeaways:\n",
        "- **Easy Setup**: Klavis MCP servers can be created with just a few lines of code\n",
        "- **Gemini Compatible**: All tools are formatted for seamless Gemini function calling\n",
        "- **Versatile**: Support for both simple APIs (YouTube) and OAuth-authenticated services (Gmail)\n",
        "- **Scalable**: The same pattern can be applied to any of the MCP servers available in Klavis\n",
        "- **Multimodal Ready**: Gemini's multimodal capabilities can be leveraged for more complex use cases\n",
        "\n",
        "**Happy building!** 🚀\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
